{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "file_path_train = r\"D:\\Download\\zyFile\\Cyberthreat_Cognitive_System\\CTCS_Code\\attack_datasets\\NSL-KDD\\KDDTrain+.txt\"\n",
    "file_path_test = r\"D:\\Download\\zyFile\\Cyberthreat_Cognitive_System\\CTCS_Code\\attack_datasets\\NSL-KDD\\KDDTest+.txt\"\n",
    "# 定义列名\n",
    "data_columns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
    "                \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
    "                \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
    "                \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "                \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
    "                \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
    "                \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
    "                \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "                \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "                \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\", \"difficulty\"]\n",
    "# 加载数据\n",
    "train_data = pd.read_csv(file_path_train, header=None, names=data_columns)\n",
    "test_data = pd.read_csv(file_path_test, header=None, names=data_columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78c8f7bf2751ba59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attack_mapping = {}\n",
    "with open(r'D:\\Download\\zyFile\\Cyberthreat_Cognitive_System\\CTCS_Code\\attack_datasets\\NSL-KDD\\attack_name',\n",
    "          'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        if len(parts) == 2:\n",
    "            attack, category = parts\n",
    "            attack_mapping[attack] = category\n",
    "# 然后像之前那样使用这个映射字典\n",
    "train_data['label'] = train_data['label'].map(attack_mapping)\n",
    "test_data['label'] = test_data['label'].map(attack_mapping)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2b09b0ed24a80a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(f'train_label is {Counter(train_data[\"label\"])}')\n",
    "print(f'test_label is {Counter(test_data[\"label\"])}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3aec8ff7f6e30a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获取训练集和测试集中的 'service' 列\n",
    "service_train = train_data['service']\n",
    "service_test = test_data['service']\n",
    "\n",
    "protocol_type_train = train_data['protocol_type']\n",
    "protocol_type_test = test_data['protocol_type']\n",
    "\n",
    "flag_train = train_data['flag']\n",
    "flag_test = test_data['flag']\n",
    "\n",
    "label_train = train_data['label']\n",
    "label_test = test_data['label']\n",
    "\n",
    "# 找出只在训练集中出现的 service 类型\n",
    "unique_service = set(service_train) - set(service_test)\n",
    "unique_protocol_type = set(protocol_type_train) - set(protocol_type_test)\n",
    "unique_flag = set(flag_train) - set(flag_test)\n",
    "unique_label = set(label_train) - set(label_test)\n",
    "\n",
    "test_unique_service = set(service_test) - set(service_train)\n",
    "test_unique_protocol_type = set(protocol_type_test) - set(protocol_type_train)\n",
    "test_unique_flag = set(flag_test) - set(flag_train)\n",
    "test_unique_label = set(label_test) - set(label_train)\n",
    "\n",
    "# 输出结果\n",
    "print(\"service type only exist in train_dataset:\", unique_service)\n",
    "print(\"protocol type only exist in train_dataset:\", unique_protocol_type)\n",
    "print(\"flag type only exist in train_dataset:\", unique_flag)\n",
    "print(\"label type only exist in train_dataset:\", unique_label)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"service type only exist in test_dataset:\", test_unique_service)\n",
    "print(\"protocol type only exist in test_dataset:\", test_unique_protocol_type)\n",
    "print(\"flag type only exist in test_dataset:\", test_unique_flag)\n",
    "print(\"label type only exist in test_dataset:\", test_unique_label)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0bafcf01f29b90d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义要检查的服务类型列表\n",
    "services_to_check = ['urh_i', 'http_2784', 'aol', 'http_8001', 'harvest', 'red_i']\n",
    "\n",
    "# 对每种服务类型进行计数\n",
    "for service in services_to_check:\n",
    "    count = (train_data['service'] == service).sum()\n",
    "    print(f\"Number of occurrences for service '{service}': {count}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62795c1cf98bb9b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 找出只在训练集中出现的 service 类型\n",
    "unique_service_train = set(train_data['service']) - set(test_data['service'])\n",
    "# 删除训练集中存在但测试集中不存在的 service 类型的行\n",
    "train_data = train_data[~train_data['service'].isin(unique_service_train)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea7a72e43106994e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'train_label is {Counter(train_data[\"label\"])}')\n",
    "print(f'test_label is {Counter(test_data[\"label\"])}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c50a5982ed2bd749"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 找出只在训练集中出现的 service 类型\n",
    "unique_service = set(train_data) - set(test_data)\n",
    "test_unique_service = set(test_data) - set(train_data)\n",
    "# 输出结果\n",
    "print(\"service type only exist in train_dataset:\", unique_service)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"service type only exist in test_dataset:\", test_unique_service)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "274855e0a13eac04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 删除train_data中的difficulty, label列\n",
    "X_train = train_data.drop('label', axis=1)\n",
    "X_train = X_train.drop('difficulty', axis=1)\n",
    "# 提取出训练集中的label标签\n",
    "labels_train = train_data['label']\n",
    "# 删掉测试集的标签项\n",
    "X_test = test_data.drop('label', axis=1)\n",
    "X_test = X_test.drop('difficulty', axis=1)\n",
    "labels_test = test_data['label']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf768789888326a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_train['protocol_type'].nunique())\n",
    "print(X_train['service'].nunique())\n",
    "print(X_train['flag'].nunique())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1606b43f6ebba582"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# 初始化LabelEncoder\n",
    "le_protocol_type = preprocessing.LabelEncoder()\n",
    "le_service = preprocessing.LabelEncoder()\n",
    "le_flag = preprocessing.LabelEncoder()\n",
    "le_labels = preprocessing.LabelEncoder()\n",
    "\n",
    "# 对训练集进行标签编码\n",
    "X_train['protocol_type'] = le_protocol_type.fit_transform(X_train['protocol_type'])\n",
    "X_train['service'] = le_service.fit_transform(X_train['service'])\n",
    "X_train['flag'] = le_flag.fit_transform(X_train['flag'])\n",
    "labels_train = le_labels.fit_transform(labels_train) + 1\n",
    "\n",
    "# 使用相同的编码器对测试集进行标签编码\n",
    "X_test['protocol_type'] = le_protocol_type.transform(X_test['protocol_type'])\n",
    "X_test['service'] = le_service.transform(X_test['service'])\n",
    "X_test['flag'] = le_flag.transform(X_test['flag'])\n",
    "labels_test = le_labels.transform(labels_test) + 1\n",
    "\n",
    "protocol_type_mapping = le_protocol_type.classes_\n",
    "service_mapping = le_service.classes_\n",
    "flag_mapping = le_flag.classes_\n",
    "label_mapping = le_labels.classes_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bc7601f2bd3117c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# \n",
    "# # 初始化 SMOTE 对象\n",
    "# smote = SMOTE()\n",
    "# \n",
    "# # 对训练集进行重采样\n",
    "# train_data, label_train = smote.fit_resample(train_data, label_train)\n",
    "# \n",
    "# # 打印重采样后的数据规模\n",
    "# print(f'重采样后的数据规模')\n",
    "# print(f'X_train_resampled shape: {train_data.shape}')\n",
    "# print(f'labels_train_resampled shape: {label_train.shape}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecc3a19762f518bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# 初始化 ADASYN 对象\n",
    "adasyn = ADASYN()\n",
    "\n",
    "# 对训练集进行重采样\n",
    "X_train, labels_train = adasyn.fit_resample(X_train, labels_train)\n",
    "\n",
    "# 打印重采样后的数据规模\n",
    "print(f'重采样后的数据规模')\n",
    "print(f'X_train_resampled shape: {X_train.shape}')\n",
    "print(f'labels_train_resampled shape: {labels_train.shape}')\n",
    "# 注意：X_test 和 labels_test 不需要重采样"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af7f2049a73cbae5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'test_label is {Counter(labels_train)}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df714a7be7110adb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 特征标准化\n",
    "scaler = preprocessing.StandardScaler()\n",
    "standard_train_X = scaler.fit_transform(X_train)\n",
    "standard_test_X = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea28fe9f4bca95dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "# selector = VarianceThreshold(threshold=1)  # 设置方差的阈值\n",
    "# X_train_selected = selector.fit_transform(standard_train_X)\n",
    "# X_test_selected = selector.transform(standard_test_X)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "565ba054460dd1c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from scipy.stats import pearsonr\n",
    "# import numpy as np\n",
    "# \n",
    "# # 定义计算皮尔森相关系数的函数\n",
    "# def pearsonr_correlation(X, y):\n",
    "#     # 计算每个特征与目标变量的相关性\n",
    "#     correlations = np.array([pearsonr(x, y)[0] for x in X.T])\n",
    "#     return correlations\n",
    "# \n",
    "# # 创建 SelectKBest 实例，选择 k 个最相关的特征\n",
    "# k = 10  # 您可以根据需要设置 k 的值\n",
    "# selector = SelectKBest(score_func=lambda X, y: pearsonr_correlation(X, y), k=k)\n",
    "# \n",
    "# # 对训练数据进行拟合和转换\n",
    "# X_train_selected = selector.fit_transform(standard_train_X, labels_train)\n",
    "# \n",
    "# # 对测试数据进行转换\n",
    "# X_test_selected = selector.transform(standard_test_X)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a48e8d7bac48b058"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# \n",
    "# # 训练 GradientBoostingClassifier 模型\n",
    "# gbc = GradientBoostingClassifier()\n",
    "# gbc.fit(standard_train_X, labels_train)\n",
    "# \n",
    "# # 创建 SelectFromModel 实例\n",
    "# selector = SelectFromModel(gbc, prefit=True)\n",
    "# \n",
    "# # 对训练和测试数据应用特征选择\n",
    "# X_train_selected = selector.transform(standard_train_X)\n",
    "# X_test_selected = selector.transform(standard_test_X)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edd8f083f27848a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'The dimension after Z-score StandardScaler: {standard_train_X.shape}')\n",
    "print(f'The dimension after Z-score StandardScaler: {standard_test_X.shape}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cdbf20c73f218f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 打印经过标准化的训练数据的前几行\n",
    "# print(standard_train_X[:1:])\n",
    "# # 计算并打印训练数据的均值和标准差\n",
    "# print(\"Train Data Mean:\", np.mean(standard_train_X, axis=0))\n",
    "# print(\"Train Data Standard Deviation:\", np.std(standard_train_X, axis=0))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93dcd2530ad7fc8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 打印经过标准化的测试数据的前几行\n",
    "# print(standard_test_X[:1, :])\n",
    "# # 计算并打印测试数据的均值和标准差\n",
    "# print(\"Test Data Mean:\", np.mean(standard_test_X, axis=0))\n",
    "# print(\"Test Data Standard Deviation:\", np.std(standard_test_X, axis=0))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b758c8185938bbff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.boxplot(data=X_train_selected)\n",
    "# plt.title('Boxplot of Standardized Training Data')\n",
    "# plt.ylim([-20, 20])\n",
    "# plt.xlabel('Feature Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "438a62aec12974d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.boxplot(data=X_test_selected)\n",
    "# plt.title('Boxplot of Standardized Testing Data')\n",
    "# plt.ylim([-20, 20])\n",
    "# plt.xlabel('Feature Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fdd9fe50400d3d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 进行 PCA 降维\n",
    "# from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # 初始化 PCA，不设置 n_components\n",
    "# pca = PCA()\n",
    "# \n",
    "# # 对数据进行拟合\n",
    "# pca.fit(X_train_selected)\n",
    "# \n",
    "# # 现在可以访问 explained_variance_ratio_\n",
    "# explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "# \n",
    "# # 绘制 Scree Plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(explained_variance, marker='o')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.title('Scree Plot')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# # 计算累积方差\n",
    "# cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "# \n",
    "# # 找到解释了至少 95% 总方差的主成分数量\n",
    "# n_components = np.where(cumulative_variance > 0.95)[0][0] + 1\n",
    "# \n",
    "# # 使用找到的 n_components 重新初始化 PCA\n",
    "# pca = PCA(n_components=n_components)\n",
    "# new_X = pca.fit_transform(X_train_selected)\n",
    "# new_test_X = pca.transform(X_test_selected)\n",
    "# print(n_components)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58fa22934cf92cc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FINAL_TRAIN = standard_train_X\n",
    "FINAL_TEST = standard_test_X"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ee0b88dc41aa59c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "# 初始化SVC模型\n",
    "svc = SVC(kernel='rbf', class_weight='balanced', C=0.5)\n",
    "\n",
    "# 训练模型\n",
    "start = time.time()\n",
    "# clf = svc.fit(standard_train_X, labels_train)  # 使用全部数据进行训练\n",
    "clf = svc.fit(FINAL_TRAIN, labels_train)  # 使用全部数据进行训练\n",
    "print('训练用时：{0}'.format(time.time() - start))\n",
    "\n",
    "# 保存模型（如果需要）\n",
    "# joblib.dump(clf, './model/IDS_model_full_data.m')\n",
    "# print('Model saved')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b686c048ebd1b17d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL = clf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "351b91bb4ae866d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "# 使用测试集进行预测\n",
    "y_pred = clf.predict(FINAL_TEST)\n",
    "\n",
    "accuracy = np.mean(y_pred == label_test)\n",
    "print(f'accuracy is {accuracy}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9abd584b9460a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 生成混淆矩阵\n",
    "conf_matrix = confusion_matrix(label_test, y_pred)\n",
    "# 获取类别名称（假设 label_mapping 是之前保存的映射）\n",
    "class_names = label_mapping\n",
    "# 可视化混淆矩阵\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79ab1564326a73f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# 获取测试集上的决策函数得分\n",
    "y_score = clf.decision_function(FINAL_TEST)\n",
    "\n",
    "# 为每个类别进行one-hot编码\n",
    "y_test_binarized = label_binarize(labels_test, classes=np.unique(labels_train))\n",
    "\n",
    "# 计算ROC曲线和ROC面积\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# 绘制所有类别的ROC曲线\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'deeppink', 'navy'])  # 五种颜色\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'.format(class_names[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-class ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b43b4a06cc775223"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# 计算精度、召回率和F1分数\n",
    "recall = recall_score(labels_test, y_pred, average='weighted')\n",
    "precision = precision_score(labels_test, y_pred, average='weighted')\n",
    "f1 = f1_score(labels_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# 定义MCC分数计算函数\n",
    "def mcc_score(y_true, y_pred, class_label):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tp = cm[class_label, class_label]\n",
    "    tn = np.sum(cm) - np.sum(cm[class_label, :]) - np.sum(cm[:, class_label]) + tp\n",
    "    fp = np.sum(cm[:, class_label]) - tp\n",
    "    fn = np.sum(cm[class_label, :]) - tp\n",
    "    numerator = (tp * tn) - (fp * fn)\n",
    "    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "# 计算每个类别的MCC并求平均\n",
    "n_classes = len(np.unique(labels_test))  # 确定类别数\n",
    "mcc_scores = [mcc_score(labels_test, y_pred, i) for i in range(n_classes)]\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "print(f\"Average MCC: {average_mcc}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0cd22ad4d369c36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 将真实标签转换为 one-hot 编码\n",
    "y_true_bin = label_binarize(labels_test, classes=np.unique(labels_train))\n",
    "\n",
    "# 确定类别数和类别名称\n",
    "n_classes = y_true_bin.shape[1]\n",
    "class_names = label_mapping  # 确保类别名称正确\n",
    "\n",
    "# 为每个类别绘制PR曲线\n",
    "for i in range(n_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_score[:, i])\n",
    "    auc_score = auc(recall, precision)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.title('Precision-Recall Curve for Class {} (AUC = {:.4f})'.format(class_names[i], auc_score))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11bd09642abc99cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tensorflow",
   "language": "python",
   "display_name": "TensorflowPy3.9.16"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
